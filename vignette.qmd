---
title: "vignette-images"
format: html
editor: visual
---

# Image Classification with CNN

## Data

Worldwide 300,000 people are diagnosed with brain tumors annually with nearly one-third of these cases being cancerous. These characteristics immediately make brain tumors a pressing issue, however brain tumors are very diverse thus would require extensive scans to diagnose specific brain tumor types. This is where image classification comes in, specifically we will be applying a convolutional neural network model. A CNN model strengths lie in it's ability to identify patterns in images and extract features from data which is great for identifying specific brain tumors. In our case, we will be dealing with the three most common types, glioma, meningioma, and pituitary tumors as well as no tumor cases. The data set we will be using contains 7023 MRI images all classified in one of the four classes. With the use of artificial intelligence it could lead to faster detections, personalized treatments, and improved planning.

```{r}
# packages
library(tidyverse)
library(tensorflow)
library(keras3)

# the data has already been partitioned
batch_size <- 32
img_height <- 256
img_width <- 256

train_tumor <- image_dataset_from_directory(
  directory = 'data/Training',
  image_size = c(img_height, img_width),
  batch_size = batch_size,
  seed = 1111
)

test_tumor <- image_dataset_from_directory(
  directory = 'data/Testing',
  image_size = c(img_height, img_width),
  batch_size = batch_size,
  seed = 1111
)
```

## Single Layer Model

First we test out the most basic neural network, which includes just a single layer. Since the data is a 3 dimensional matrix of numbers, we need to flatten it before feeding it into the model.

### Define model architecture
```{r}
model_single_layer <- keras_model_sequential(input_shape = c(img_height, img_width, 3)) %>%
  layer_flatten() %>%
  layer_dense(4) %>%
  layer_activation(activation = 'softmax')

summary(model_single_layer)
```

### Compile model
The adam optimizer is good for general use neural network training, so it should work for our model. The crossentropy loss function is typically used for classification problems, and the neural network's objective is to minimize the value of this function. The optimizer will use the loss function's gradient to try to find the minimum. Finally, we use accuracy to judge the effectiveness of the model, which is simply the number of correct predictions divided by the number of total predictions.

```{r}
model_single_layer %>% compile(
  optimizer = optimizer_adam(),
  loss = 'categorical_crossentropy',
  metrics = 'accuracy'
)
```

### Train model

```{r}
history_single_layer <- model_single_layer %>% fit(
  train_tumor,
  epochs = 20
)
```
(train_tumor, epochs = 20)

### Test
```{r}
evaluate(model_single_layer, test_tumor)
```

This accuracy is not bad for a single layer model, but there is a lot of room for improvement. The testing accuracy is lower than our training accuracy, which is a sign that the model is overfitting the training data.












