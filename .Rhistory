#install.packages("imager")  # For image processing (optional)
library(keras)
library(tensorflow)
library(imager)
setwd('~/Pstat 197/vignette-image/data')
# Set paths for datasets
train_dir <- "data/Training"
test_dir <- "data/Testing"
class_names <- c('notumor',
'pituitary',
'meningioma',
'glioma')
# Load datasets
train_dataset <- image_dataset_from_directory(
directory = train_dir,
labels = "inferred",               # Labels inferred from subdirectories
label_mode = "categorical",        # One-hot encode labels
batch_size = 32,                   # Batch size
image_size = c(128, 128)           # Resize all images to 128x128
)
test_dataset <- image_dataset_from_directory(
directory = test_dir,
labels = "inferred",
label_mode = "categorical",
batch_size = 32,
image_size = c(128, 128)
)
train_dataset <- train_dataset %>%
dataset_map(~ list(.x / 255, .y))  # Normalize pixel values to [0, 1]
install.packages("imager")  # For image processing (optional)
install.packages("imager")
library(imager)
train_dataset <- train_dataset %>%
dataset_map(~ list(.x / 255, .y))  # Normalize pixel values to [0, 1]
test_dataset <- test_dataset %>%
tf$dataset_map(~ list(.x / 255, .y))
test_dataset <- test_dataset %>%
imager$dataset_map(~ list(.x / 255, .y))
data_augmentation <- keras_model_sequential() %>%
layer_random_flip("horizontal") %>%
layer_random_rotation(0.2)
cnn_model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = 'relu', input_shape = c(128, 128, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(0.5) %>%
layer_dense(units = 2, activation = 'softmax')  # Adjust units for your number of classes
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
# Train the model
history <- model %>% fit(
train_generator,
steps_per_epoch = as.integer(train_generator$samples / batch_size),
epochs = 30,
validation_data = validation_generator,
validation_steps = as.integer(validation_generator$samples / batch_size)
)
# Train the model
history <- cnn_model %>% fit(
train_generator,
steps_per_epoch = as.integer(train_generator$samples / batch_size),
epochs = 30,
validation_data = validation_generator,
validation_steps = as.integer(validation_generator$samples / batch_size)
)
# Normalize datasets using dataset_map
train_dataset <- train_dataset %>%
dataset_map(function(x, y) {
list(x / 255, y)
})
